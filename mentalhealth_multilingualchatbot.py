# -*- coding: utf-8 -*-
"""MentalHealth_MultilingualChatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FfyaSAyNdjbLjw2JvYYzJGLYxBC_RX5

# **Install Required Libraries**
"""

!pip install llama-index llama-index-core llama-index-embeddings-huggingface pinecone

# pcsk_5VWi9Y_Ejpj6tvvsYkskoiqBL8PyHzoVCUpxdiHs24XyBefeZz6GtEVStgv9SXVXG4ftun
# multilingual-chatbot

"""# **Import Necessary Modules**"""

from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from pinecone import Pinecone

"""# **Configure Pinecone**"""

PINECONE_API_KEY = "pcsk_5VWi9Y_Ejpj6tvvsYkskoiqBL8PyHzoVCUpxdiHs24XyBefeZz6GtEVStgv9SXVXG4ftun"
PINECONE_ENV = "us-east-1-aws"
#pinecone_index_name = "multilingual-chatbot"

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
#index = pc.Index(pinecone_index_name)

from pinecone import ServerlessSpec

index_name = "mental-health-chatbot"

# Delete index if exists (optional reset)
if index_name in pc.list_indexes().names():
    pc.delete_index(index_name)

# Create Pinecone index with correct dimension (384 for both models)
pc.create_index(index_name, dimension=384, metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"))
index = pc.Index(index_name)

"""# **Load Multilingual PDFs (English + Urdu)**"""

documents = SimpleDirectoryReader(input_files=[
    "/content/mental_health_en.pdf",
    "/content/mental_health_ur.pdf"
]).load_data()

"""# **Parse and Chunk Text**

**Step A: Chunking Strategy 1 – Fixed-Length Chunking**
"""

node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=30)
nodes = node_parser.get_nodes_from_documents(documents)

"""**Step B: Chunking Strategy 2 – Sentence-Based Chunking**"""

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

# Sentence-based chunking (e.g., 3 sentences per chunk)
def sentence_based_chunking(text, n=3):
    sentences = sent_tokenize(text)
    chunks = [' '.join(sentences[i:i+n]) for i in range(0, len(sentences), n)]
    return chunks

# Apply on each document
sentence_chunks = []
for doc in documents:
    text = doc.text
    chunks = sentence_based_chunking(text, n=3)
    sentence_chunks.extend(chunks)


print(f"Total sentence-based chunks: {len(sentence_chunks)}")

"""# **Embed with All Three Models And Upload For Fixed Length Chunk**

**1. Sentence-BERT Embedding and Upload**
"""

sbert_model = HuggingFaceEmbedding("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
sbert_upsert = []
for i, node in enumerate(nodes):
    text = node.get_content()
    node.embedding = sbert_model.get_text_embedding(text)
    node.metadata = {
        "source_text": text[:300],
        "language": "urdu" if "ہے" in text or "کو" in text else "english",
        "model": "sentence-bert",
        "chunking": "fixed"
    }
    sbert_upsert.append({
        "id": f"sbert-{i}",
        "values": node.embedding,
        "metadata": node.metadata
    })
index.upsert(vectors=sbert_upsert)
print("✅ SBERT embeddings stored successfully!")

"""**2. DistilBERT Embedding and Upload**"""

index_name2 = "distilbert-index"

# Delete index if exists (optional reset)
if index_name2 in pc.list_indexes().names():
    pc.delete_index(index_name2)

# Create Pinecone index with correct dimension
pc.create_index(index_name2, dimension=768, metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"))
index = pc.Index(index_name2)

distil_model = HuggingFaceEmbedding("distilbert-base-multilingual-cased")
distil_upsert = []
for i, node in enumerate(nodes):
    text = node.get_content()
    node.embedding = distil_model.get_text_embedding(text)
    node.metadata = {
        "source_text": text[:300],
        "language": "urdu" if "ہے" in text or "کو" in text else "english",
        "model": "distilbert",
        "chunking": "fixed"
    }
    distil_upsert.append({
        "id": f"distil-{i}",
        "values": node.embedding,
        "metadata": node.metadata
    })
distil_index.upsert(vectors=distil_upsert)
print("✅ DistilBERT embeddings stored successfully!")

"""# **Similarity Search Testing Script**"""

def similarity_search(query, model_name="sbert", top_k=3, language=None):
    if model_name == "sbert":
        embedding = sbert_model.get_text_embedding(query)
        index = sbert_index
    elif model_name == "distilbert":
        embedding = distil_model.get_text_embedding(query)
        index = distil_index
    else:
        raise ValueError("Unsupported model")

    filter_dict = {"language": language} if language else None

    results = index.query(
        vector=embedding,
        top_k=top_k,
        include_metadata=True,
        filter=filter_dict
    )

    print(f"\n🧠 Top {top_k} results for query: '{query}' using {model_name.upper()}:\n")
    for i, match in enumerate(results['matches']):
        print(f"Result #{i+1} | Score: {match['score']:.4f}")
        print("Chunk:", match['metadata']['source_text'])
        print("-" * 60)

sbert_index = pc.Index("mental-health-chatbot")
distil_index = pc.Index("distilbert-index")

# For Sentence-BERT (SBERT)
print("SBERT -> ENGLISH")
similarity_search("What are symptoms of anxiety?", model_name="sbert", language="english")
print("SBERT -> URDU")
similarity_search("ذہنی دباؤ کی علامات کیا ہیں؟", model_name="sbert", language="urdu")

# For DistilBERT
print("DistilBERT -> ENGLISH")
similarity_search("What are symptoms of anxiety?", model_name="distilbert", language="english")
print("DistilBERT -> URDU")
similarity_search("ذہنی دباؤ کی علامات کیا ہیں؟", model_name="distilbert", language="urdu")

